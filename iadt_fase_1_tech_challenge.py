# -*- coding: utf-8 -*-
"""IADT - Fase 1 - Tech Challenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_1-Zoe86lthWImk7B0zs7mnBu-KotZgX

# Analise de dados
"""

# Contextualizar como foi gerado e quais colunas foram acrescentadas
import pandas as pd
df = pd.read_csv('/content/base_custos_medicos_final.csv')
df.head()

# Analise das colunas com seus tipos
df.info()

df.shape

df.describe()

# Frequência de categorias
print("Gênero:", df['genero'].value_counts())
print("Fumante:", df['fumante'].value_counts())
print("Região:", df['regiao'].value_counts())
print("Atividade Física:", df['nivel_atividade'].value_counts())
print("Tem pressão alta:", df['pressao_alta'].value_counts())
print("Tem colesterol alto:", df['colesterol_alto'].value_counts())
print("Frequenta checkup:", df['frequenta_checkup'].value_counts())
print("Álcool:", df['ingestao_alcool'].value_counts())

# Verifica quantos valores nulos existem em cada coluna
df.isnull().sum()

# Indentificar outliers visualmente
import matplotlib.pyplot as plt

# Remover valores nulos da coluna 'imc'
imc_sem_nan = df['imc'].dropna()

# Plotar boxplot
plt.figure(figsize=(6, 4))
plt.boxplot(imc_sem_nan)
plt.title('Boxplot do IMC')
plt.ylabel('IMC')
plt.grid(True)
plt.show()

# Existem outliers, vamos tratar os dados nulos de IMC com medina(melhor pratica quando existe outliers)
df['imc'] = df['imc'].fillna(df['imc'].median())

# Remover valores nulos da coluna 'nivel_estresse'
nivel_estresse_sem_nan = df['nivel_estresse'].dropna()

# Plotar boxplot
plt.figure(figsize=(6, 4))
plt.boxplot(nivel_estresse_sem_nan)
plt.title('Boxplot do Stress')
plt.ylabel('STRESS')
plt.grid(True)
plt.show()

# Não existem outliers, podemos aplicar para tratar os dados a media.
df['nivel_estresse'] = df['nivel_estresse'].fillna(df['nivel_estresse'].mean())

# As colunas nivel_atividade e ingestao_alcool, são categóricas. Vamos explicitar a ausência.
df['nivel_atividade'] = df['nivel_atividade'].fillna('desconhecido')
df['ingestao_alcool'] = df['ingestao_alcool'].fillna('desconhecido')

# Vamos verificar se existem dados nulos ainda.
df.isnull().sum()

# Frequência de categorias
print("Atividade Física:", df['nivel_atividade'].value_counts())
print("Álcool:", df['ingestao_alcool'].value_counts())

# Codificar variáveis categóricas
df.select_dtypes(include='object').columns

# Label Encoding
df['genero'] = df['genero'].map({'masculino': 0, 'feminino': 1})
df['fumante'] = df['fumante'].map({'nao': 0, 'sim': 1})
df['pressao_alta'] = df['pressao_alta'].map({'nao': 0, 'sim': 1})
df['colesterol_alto'] = df['colesterol_alto'].map({'nao': 0, 'sim': 1})
df['frequenta_checkup'] = df['frequenta_checkup'].map({'nao': 0, 'sim': 1})

df.head()

# One Hot Encoding
new_df = pd.get_dummies(df, columns=['regiao', 'nivel_atividade', 'ingestao_alcool'], drop_first=True)

new_df.head()

from sklearn.preprocessing import StandardScaler

# Selecionar apenas colunas numéricas
colunas_numericas = new_df.select_dtypes(include=['int64', 'float64']).columns

# Criar uma cópia separada para escalar
scaler = StandardScaler()
new_df[colunas_numericas] = scaler.fit_transform(new_df[colunas_numericas])

new_df.describe()

new_df.head()

import seaborn as sns

# Calcular correlação
correlacao = new_df.corr(numeric_only=True)

# Filtrar apenas as que têm correlação significativa com 'encargos'
limiar = 0.1
variaveis_relevantes = correlacao['encargos'][abs(correlacao['encargos']) > limiar].index

# Criar nova matriz de correlação com apenas essas variáveis
correlacao_reduzida = new_df[variaveis_relevantes].corr()

# Plotar
plt.figure(figsize=(8, 6))
sns.heatmap(correlacao_reduzida, annot=True, cmap='coolwarm', fmt='.2f')
plt.title("Matriz de Correlação Focada nas Variáveis Relevantes")
plt.show()

# Correlação com encargos
corre_com_encargos = correlacao['encargos'].drop('encargos').sort_values(ascending=False)

# Plot
plt.figure(figsize=(10, 5))
sns.barplot(x=corre_com_encargos.values, y=corre_com_encargos.index)
plt.title('Correlação de Variáveis com Encargos')
plt.xlabel('Correlação')
plt.ylabel('Coluna')
plt.show()

sns.histplot(df['encargos'], kde=True)
plt.title('Distribuição dos Encargos Médicos')
plt.show()

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Separar os dados
X = new_df.drop('encargos', axis=1)
y = new_df['encargos']

# Dividir em treino (80%) e teste (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Regressão Linear"""

# Criar e treinar o modelo
modelo_lr = LinearRegression()
modelo_lr.fit(X_train, y_train)

# Previsões no conjunto de teste
y_pred = modelo_lr.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Erros
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R²: {r2:.2f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.xlabel("Valor Real")
plt.ylabel("Valor Previsto")
plt.title("Regressão Linear: Valor Real vs Valor Previsto")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.show()

"""# Árvore de Decisão Regressora"""

from sklearn.tree import DecisionTreeRegressor

# Criar o modelo definindo hiperparâmetros
arvore = DecisionTreeRegressor(
    criterion='squared_error',
    max_depth=5,        # profundidade máxima da árvore (ajuste inicial)
    min_samples_leaf=10, # mínimo de amostras em cada folha
    random_state=42
)

# Treinar a Árvore de Decisão Regressora
arvore.fit(X_train, y_train)

# Previsões no conjunto de teste
y_pred_arvore = arvore.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

mse_arvore = mean_squared_error(y_test, y_pred_arvore)
rmse_arvore = np.sqrt(mse_arvore)
r2_arvore = r2_score(y_test, y_pred_arvore)

print(f"Árvore de Decisão – MSE: {mse_arvore:.2f}")
print(f"Árvore de Decisão – RMSE: {rmse_arvore:.2f}")
print(f"Árvore de Decisão – R²: {r2_arvore:.2f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_arvore, alpha=0.7, color='green')
plt.xlabel("Valor Real")
plt.ylabel("Valor Previsto (Árvore)")
plt.title("Árvore de Decisão Regressora: Real vs Previsto")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.show()

"""# Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor

# Criar o modelo básico
rf = RandomForestRegressor(
    n_estimators=100,      # número de árvores no grupo (100 é bom ponto de partida)
    max_depth=5,           # mesma profundidade da árvore anterior para compararmos
    min_samples_leaf=10,   # mesma restrição mínima de folhas
    random_state=42
)

# Treinar
rf.fit(X_train, y_train)

# Previsões no conjunto de teste
y_pred_rf = rf.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest – MSE: {mse_rf:.2f}")
print(f"Random Forest – RMSE: {rmse_rf:.2f}")
print(f"Random Forest – R²: {r2_rf:.2f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.7, color='orange')
plt.xlabel("Valor Real")
plt.ylabel("Valor Previsto (Random Forest)")
plt.title("Random Forest Regressor: Real vs Previsto")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.show()

"""# Calcular Resíduos"""

# Calcular Resíduos
residuos = y_test - y_pred
plt.figure(figsize=(8, 6))
plt.scatter(y_pred, residuos, alpha=0.6)
plt.hlines(0, y_pred.min(), y_pred.max(), colors='r', linestyles='--')
plt.xlabel("Valor Ajustado (Previsto)")
plt.ylabel("Resíduo (Real – Previsto)")
plt.title("Resíduos vs. Valores Ajustados")
plt.show()

# homocedasticidade, validando nossa regressão linear